This is a log of one programmers study and exploration of machine learning technology and what it all means. The goal is to remember what I did and how, instead of insightful observations.

## 2/17/2017
[Tensorflow 1.0](https://research.googleblog.com/2017/02/announcing-tensorflow-10.html) was announced this Tuesday during the 2017 Dev Summit. I've been slowly watching the [released videos](https://www.youtube.com/playlist?list=PLOU2XLYxmsIKGc_NBoIhTn2Qhraji53cv). I've been surprised to see how many new capabilities have been added in the past several months, including the ability to compile models to optimized ARM assembly for running on a mobile device using [XLA compiler](https://www.youtube.com/watch?v=kAOanJczHA0), Windows support and a Keras high-level API.

To celebrate the occasion I managed to install version 1.0 on both my Mac Book Pro and an Ubuntu [AWS P2 instance](https://aws.amazon.com/ec2/instance-types/p2/). On macOS I had to upgrade to Cuda SDK 8.0, and then debug why Tensorflow couldn't find the cuda libraries. The solution was to make sure that both `LD_LIBRARY_PATH` and `DYLD_LIBRARY_PATH` environment variables included the cuda library paths (`/usr/local/cuda/lib` in my case). Before this fix `import tensorflow` would cause python to crash without any errors, so I had to learn how to run python in a debugger: `lldb -f python -- test.py`.

The AWS P2 instance was setup using the [setup_p2.sh script](https://github.com/fastai/courses/blob/master/setup/setup_p2.sh) from the [fast.ai course](http://course.fast.ai/lessons/aws.html). This image was setup with [Anaconda Python](https://www.continuum.io/downloads), [Theano](http://deeplearning.net/software/theano/) and [Keras](https://keras.io). The Tensorflow installation was as simple as `pip install tensorflow_gpu`. My test code started running on an Nvidia K80 GPU right away at $0.90/hr. I also have to pay ~$12/month for storing a 128GB disk image.

## 1/29/2017

I finally found some time this weekend to work through the notebooks and assignments from the [fast.ai course](http://course.fast.ai) this weekend. First I had to request access to AWS GPU instances which tooks about 2 days to complete. Setting up my first P2 instance took less than two an hour following the [AWS Deep Learning setup video](http://course.fast.ai/lessons/aws.html).

I took a detour to learn and play with [Jupyter Notebooks](http://jupyter-notebook.readthedocs.io/en/latest/notebook.html) and [Google Datalab](https://cloud.google.com/datalab). On Sunday I started going through the [fast.ai lesson notebooks](https://github.com/fastai/courses/blob/master/deeplearning1/nbs/lesson1.ipynb). The first notebook shows you how to re-training the [VGG16 CNN model](http://www.robots.ox.ac.uk/~vgg/research/very_deep/0) to classify images of cats and dogs. This is not a trvial NN example, or even the common MINST classifier many tutorials start with.

I don't yet fully understand the details of how the VGG model works, or what it would take to train a model of my own. However, having a fully working example makes it easier to explore and learn with confidence. It should be possible to solve useful problems just by re-assembling and copying existing open source models.
